{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E-mNhUjQuxNM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N9lmLw8cuxNN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def is_cuda():\n",
        "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
        "\n",
        "is_cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eUMlpjFJuxNO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def is_hip_mi200():\n",
        "    target = triton.runtime.driver.active.get_current_target()\n",
        "    return target.backend == 'hip' and target.arch == 'gfx90a'\n",
        "\n",
        "is_hip_mi200()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lBNGYaejuxNO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PA2 Part 2: MatMul+Relu+Add Fused Optimization.\n",
        "The kernel uses several optimization techniques:\n",
        "\n",
        "  1. Shared memory tiling.\n",
        "  2. Register tiling.\n",
        "  3. Cooperative fetching.\n",
        "  4. Operator Fusion\n",
        "  5. Write cache / epilogue fusion.\n",
        "\n",
        "Fill in the missing parts (marked with TODO).\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Tiling parameters - You will need to change these to achieve better results.\n",
        "# -----------------------------------------------------------------------------\n",
        "BLOCK_M = 128  # Tile size in the M dimension.\n",
        "BLOCK_N = 128 # Tile size in the N dimension.\n",
        "BLOCK_K = 32 # Tile size in the K dimension.\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Triton Kernel: Matrix Multiplication + ReLU + Add\n",
        "#\n",
        "# The kernel uses:\n",
        "#   Step 1: Tile assignment (each kernel computes a tile of C)\n",
        "#   Step 2: Shared memory tiling + Cooperative Fetching: Load tiles of A and B.\n",
        "#   Step 3: Register tiling: Use a register accumulator.\n",
        "#   Step 4: Add and ReLU fusion\n",
        "#   Step 5: Write cache/Epilogue: Write the final tile back to global memory.\n",
        "# -----------------------------------------------------------------------------\n",
        "@triton.jit\n",
        "def matmul_add_relu_kernel_fp16(\n",
        "    a_ptr, b_ptr, c_ptr, d_ptr,\n",
        "    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n",
        "    stride_am: tl.constexpr, stride_ak: tl.constexpr,\n",
        "    stride_bk: tl.constexpr, stride_bn: tl.constexpr,\n",
        "    stride_cm: tl.constexpr, stride_cn: tl.constexpr,\n",
        "    stride_dm: tl.constexpr, stride_dn: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 1: Tile: Assignment\n",
        "    #\n",
        "    # Each kernel instance is mapped to a tile in the output matrix C.\n",
        "    # Compute the starting indices (m_start, n_start) for this tile.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Compute the tile indices using program_id(0) for M and program_id(1) for N.\n",
        "    grid_m = tl.program_id(0)\n",
        "    grid_n = tl.program_id(1)\n",
        "    m_start = grid_m * BLOCK_M\n",
        "    n_start = grid_n * BLOCK_N\n",
        "    off_m = m_start + tl.arange(0, BLOCK_M)\n",
        "    off_n = n_start + tl.arange(0, BLOCK_N)\n",
        "    mask_m = off_m < M\n",
        "    mask_n = off_n < N\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 2: Register Tiling\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Initialize the accumulator \"acc\" with zeros (dtype: float16).\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float16)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 3: Shared Memory Tiling & Cooperative Fetching.\n",
        "    # Compute pointers to the sub-tiles of A and B that are needed to compute\n",
        "    # the current C tile. The offsets here serve to load BLOCK_SIZE_M x BLOCK_SIZE_K\n",
        "    # and BLOCK_SIZE_K x BLOCK_SIZE_N blocks from A and B respectively.\n",
        "    # -------------------------------------------------------------------------\n",
        "    k_iterations = tl.cdiv(K, BLOCK_K)\n",
        "    for k in range(0, k_iterations):\n",
        "        k_start = k * BLOCK_K\n",
        "        off_k = k_start + tl.arange(0, BLOCK_K)\n",
        "        mask_k = off_k < K\n",
        "\n",
        "        a_ptrs = a_ptr + off_m[:, None] * stride_am + off_k[None, :] * stride_ak\n",
        "        b_ptrs = b_ptr + off_k[:, None] * stride_bk + off_n[None, :] * stride_bn\n",
        "        a_mask = mask_m[:, None] & mask_k[None, :]\n",
        "        b_mask = mask_k[:, None] & mask_n[None, :]\n",
        "        a = tl.load(a_ptrs, mask=a_mask, other=0)\n",
        "        b = tl.load(b_ptrs, mask=b_mask, other=0)\n",
        "\n",
        "        acc += tl.dot(a, b, out_dtype=tl.float16)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 4: Apply ReLU and Add C to the accumulator\n",
        "    # -------------------------------------------------------------------------\n",
        "    c_ptrs = c_ptr + off_m[:, None] * stride_cm + off_n[None, :] * stride_cn\n",
        "    c_mask = mask_m[:, None] & mask_n[None, :]  \n",
        "    c = tl.load(c_ptrs, mask=c_mask)\n",
        "    acc = acc + c\n",
        "    acc = tl.maximum(acc, 0)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 5: Write Cache / Epilogue Fusion: Write the computed tile to D.\n",
        "    # -------------------------------------------------------------------------\n",
        "    d_ptrs = d_ptr + off_m[:, None] * stride_dm + off_n[None, :] * stride_dn\n",
        "    d_mask = mask_m[:, None] & mask_n[None, :]\n",
        "    tl.store(d_ptrs, acc, mask=d_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u16sz-IUuxNP"
      },
      "outputs": [],
      "source": [
        "def matmul_add_relu_fp16(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor,\n",
        "                         block_m: int = BLOCK_M, block_n: int = BLOCK_N, block_k: int = BLOCK_K) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n",
        "    \"\"\"\n",
        "    M, K = a.shape\n",
        "    K2, N = b.shape\n",
        "    assert K == K2, \"Incompatible dimensions\"\n",
        "\n",
        "    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    # Create launch grid\n",
        "    grid = (triton.cdiv(M, block_m), triton.cdiv(N, block_n))\n",
        "\n",
        "    matmul_add_relu_kernel_fp16[grid](\n",
        "        a, b, c, d,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "        d.stride(0), d.stride(1),\n",
        "        BLOCK_M=block_m, BLOCK_N=block_n, BLOCK_K=block_k\n",
        "    )\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AJ7LlTPawPqB"
      },
      "outputs": [],
      "source": [
        "# Reference implementation using PyTorch\n",
        "def reference_matmul_add_relu(A, B, C):\n",
        "    result = torch.matmul(A, B).add(C).relu_()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4J5ZBpOuxNP",
        "outputId": "d8e8dcd0-f386-43bd-88e8-bf4ca365ae54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "triton_output_with_fp16_inputs=tensor([[ 3.4297,  0.0000, 12.4453,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [23.2656,  0.0000,  0.0000,  ...,  0.0000,  0.0000, 27.6250],\n",
            "        [ 0.0000,  0.9302,  0.0000,  ..., 10.3906,  0.0000, 14.1016],\n",
            "        ...,\n",
            "        [14.2578,  0.0000, 10.1953,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000, 27.2812,  0.0000,  ...,  4.6367,  0.0000,  0.0000],\n",
            "        [ 0.0000, 20.9375,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "torch_output_with_fp16_inputs=tensor([[ 3.4336,  0.0000, 12.4453,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [23.3281,  0.0000,  0.0000,  ...,  0.0000,  0.0000, 27.5938],\n",
            "        [ 0.0000,  0.9146,  0.0000,  ..., 10.3750,  0.0000, 14.0938],\n",
            "        ...,\n",
            "        [14.2578,  0.0000, 10.2031,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000, 27.2344,  0.0000,  ...,  4.6172,  0.0000,  0.0000],\n",
            "        [ 0.0000, 20.9531,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "✅ Triton and Torch match\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Accuracy Tests\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    a = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    b = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    c = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "    torch_output = reference_matmul_add_relu(a, b, c)\n",
        "    print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
        "    print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
        "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "        print(\"✅ Triton and Torch match\")\n",
        "    else:\n",
        "        diff = triton_output - torch_output\n",
        "        abs_diff = torch.abs(diff)\n",
        "        max_abs_diff = torch.max(abs_diff)\n",
        "        print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj_dGOlazQJY",
        "outputId": "bad84c96-6624-4a21-f1b4-c014a226d709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.24 ms\n",
            "PyTorch implementation: 0.45 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.85x\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Performance Benchmark \n",
        "# IMPORTANT: DO NOT CHANGE THIS CODE. \n",
        "# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n",
        "# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n",
        "# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n",
        "# -----------------------------------------------------------------------------\n",
        "M = 2048\n",
        "K = 2048\n",
        "N = 2048\n",
        "\n",
        "# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n",
        "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# warmup\n",
        "_ = matmul_add_relu_fp16(A, B, C)\n",
        "_ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "REPEATS = 5000\n",
        "\n",
        "# time your implementation\n",
        "print(\"Triton implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = matmul_add_relu_fp16(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "# time pytorch\n",
        "print(\"PyTorch implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = reference_matmul_add_relu(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n",
        "print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n",
        "print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n",
        "\n",
        "print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K9Hdpxic0tq6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block size: 64x64x16, Triton time: 0.36 ms\n",
            "Block size: 64x64x32, Triton time: 0.36 ms\n",
            "Block size: 64x64x64, Triton time: 0.36 ms\n",
            "Block size: 64x128x16, Triton time: 0.29 ms\n",
            "Block size: 64x128x32, Triton time: 0.28 ms\n",
            "Block size: 64x128x64, Triton time: 0.30 ms\n",
            "Block size: 64x256x16, Triton time: 0.28 ms\n",
            "Block size: 64x256x32, Triton time: 0.26 ms\n",
            "Block size: 64x256x64, Triton time: 0.29 ms\n",
            "Block size: 128x64x16, Triton time: 0.30 ms\n",
            "Block size: 128x64x32, Triton time: 0.28 ms\n",
            "Block size: 128x64x64, Triton time: 0.30 ms\n",
            "Block size: 128x128x16, Triton time: 0.28 ms\n",
            "Block size: 128x128x32, Triton time: 0.24 ms\n",
            "Block size: 128x128x64, Triton time: 0.26 ms\n",
            "Block size: 128x256x16, Triton time: 0.25 ms\n",
            "Block size: 128x256x32, Triton time: 0.25 ms\n",
            "Block size: 128x256x64, Triton time: 0.28 ms\n",
            "Block size: 256x64x16, Triton time: 0.34 ms\n",
            "Block size: 256x64x32, Triton time: 0.26 ms\n",
            "Block size: 256x64x64, Triton time: 0.28 ms\n",
            "Block size: 256x128x16, Triton time: 0.26 ms\n",
            "Block size: 256x128x32, Triton time: 0.25 ms\n",
            "Block size: 256x128x64, Triton time: 0.36 ms\n",
            "Block size: 256x256x16, Triton time: 0.93 ms\n",
            "Block size: 256x256x32, Triton time: 7.62 ms\n"
          ]
        },
        {
          "ename": "OutOfResources",
          "evalue": "out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfResources\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block_n \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m]:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block_k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m]:\n\u001b[0;32m---> 25\u001b[0m         triton_time \u001b[38;5;241m=\u001b[39m \u001b[43mperf_triton_matmul_add_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlock size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Triton time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtriton_time\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m triton_time \u001b[38;5;241m<\u001b[39m best_time:\n",
            "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mperf_triton_matmul_add_relu\u001b[0;34m(M, K, N, block_m, block_n, block_k)\u001b[0m\n\u001b[1;32m      6\u001b[0m C \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((M, N), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# warmup\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul_add_relu_fp16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m REPEATS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
            "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mmatmul_add_relu_fp16\u001b[0;34m(a, b, c, block_m, block_n, block_k)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create launch grid\u001b[39;00m\n\u001b[1;32m     12\u001b[0m grid \u001b[38;5;241m=\u001b[39m (triton\u001b[38;5;241m.\u001b[39mcdiv(M, block_m), triton\u001b[38;5;241m.\u001b[39mcdiv(N, block_n))\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmatmul_add_relu_kernel_fp16\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_M\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_N\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_k\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
            "File \u001b[0;32m~/micromamba/envs/torch_gpu/lib/python3.9/site-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/micromamba/envs/torch_gpu/lib/python3.9/site-packages/triton/runtime/jit.py:691\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# launch kernel\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     launch_metadata \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mlaunch_metadata(grid, stream, \u001b[38;5;241m*\u001b[39mnon_constexpr_vals)\n\u001b[0;32m--> 691\u001b[0m     \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m(grid_0, grid_1, grid_2, stream, kernel\u001b[38;5;241m.\u001b[39mfunction, kernel\u001b[38;5;241m.\u001b[39mpacked_metadata, launch_metadata,\n\u001b[1;32m    692\u001b[0m                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_enter_hook, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_exit_hook, \u001b[38;5;241m*\u001b[39mnon_constexpr_vals)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kernel\n",
            "File \u001b[0;32m~/micromamba/envs/torch_gpu/lib/python3.9/site-packages/triton/compiler/compiler.py:381\u001b[0m, in \u001b[0;36mCompiledKernel.__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 381\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n",
            "File \u001b[0;32m~/micromamba/envs/torch_gpu/lib/python3.9/site-packages/triton/compiler/compiler.py:374\u001b[0m, in \u001b[0;36mCompiledKernel._init_handles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m max_shared \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mactive\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_device_properties(device)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_shared_mem\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mshared \u001b[38;5;241m>\u001b[39m max_shared:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutOfResources(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mshared, max_shared, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared memory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# TODO: n_regs, n_spills should be metadata generated when calling `ptxas`\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_regs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_spills \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mactive\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mload_binary(\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mshared, device)\n",
            "\u001b[0;31mOutOfResources\u001b[0m: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help."
          ]
        }
      ],
      "source": [
        "# Write your grid search here.\n",
        "\n",
        "def perf_triton_matmul_add_relu(M, K, N, block_m, block_n, block_k):\n",
        "    A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "    B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "    C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "    # warmup\n",
        "    _ = matmul_add_relu_fp16(A, B, C, block_m, block_n, block_k)\n",
        "\n",
        "    REPEATS = 5000\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(REPEATS):\n",
        "        _ = matmul_add_relu_fp16(A, B, C, block_m, block_n, block_k)\n",
        "    torch.cuda.synchronize()\n",
        "    triton_time = (time.perf_counter() - start) / REPEATS\n",
        "    return triton_time\n",
        "\n",
        "best_time = float('inf')\n",
        "best_block_mnk = 0, 0, 0\n",
        "for block_m in [64, 128, 256]:\n",
        "    for block_n in [64, 128, 256]:\n",
        "        for block_k in [16, 32, 64]:\n",
        "            triton_time = perf_triton_matmul_add_relu(M, K, N, block_m, block_n, block_k)\n",
        "            print(f\"Block size: {block_m}x{block_n}x{block_k}, Triton time: {triton_time*1000:.2f} ms\")\n",
        "            if triton_time < best_time:\n",
        "                best_time = triton_time\n",
        "                best_block_mnk = block_m, block_n, block_k\n",
        "\n",
        "print(f\"Best block size: {best_block_mnk}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
